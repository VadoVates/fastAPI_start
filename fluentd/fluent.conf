# Fluentd configuration for collecting logs from FastAPI and PostgreSQL

# Source: Docker container logs (FastAPI + PostgreSQL)
<source>
  @type tail
  @id docker_logs
  path /var/lib/docker/containers/*/*.log
  pos_file /var/log/fluentd-docker.log.pos
  tag docker.*
  format json
  read_from_head true
</source>

# Source: FastAPI application logs (if you write logs to files)
<source>
  @type tail
  @id fastapi_logs
  path /var/log/fastapi/*.log
  pos_file /var/log/fluentd-fastapi.log.pos
  tag fastapi.*
  format none
  read_from_head true
</source>

# Filter: Parse FastAPI logs and add metadata
<filter docker.**>
  @type parser
  key_name log
  reserve_data true
  <parse>
    @type json
  </parse>
</filter>

# Filter: Add container name and service labels
<filter docker.**>
  @type record_transformer
  <record>
    container_name ${record["attrs"]["name"]}
    service_type ${record["attrs"]["labels"]["com.example.service"] || "unknown"}
  </record>
</filter>

# Output: Send FastAPI logs to stdout (you can change this to Elasticsearch, S3, etc.)
<match docker.**>
  @type copy

  # Console output for debugging
  <store>
    @type stdout
    <format>
      @type json
    </format>
  </store>

  # File output for persistence
  <store>
    @type file
    path /var/log/aggregated/app
    time_slice_format %Y%m%d_%H
    compress gzip
    <format>
      @type json
    </format>
  </store>
</match>

# Output: FastAPI specific logs
<match fastapi.**>
  @type copy

  <store>
    @type stdout
    <format>
      @type json
    </format>
  </store>

  <store>
    @type file
    path /var/log/aggregated/fastapi
    time_slice_format %Y%m%d_%H
    compress gzip
    <format>
      @type json
    </format>
  </store>
</match>

# Optional: Send logs to external systems
# Uncomment and configure as needed:

# # Send to Elasticsearch
# <match **>
#   @type elasticsearch
#   host elasticsearch
#   port 9200
#   index_name temperature_monitoring
#   type_name logs
# </match>

# # Send to AWS S3
# <match **>
#   @type s3
#   aws_key_id YOUR_AWS_ACCESS_KEY
#   aws_sec_key YOUR_AWS_SECRET_KEY
#   s3_bucket your-log-bucket
#   s3_region us-east-1
#   path logs/
#   time_slice_format %Y/%m/%d/%H
# </match>